{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will explore and augment a custom dataset for entity recognition of cryptocurrencies scraped from reddit using my other project Dragonfly.\n",
    "\n",
    "https://huggingface.co/datasets/conll2003\n",
    "https://www.freecodecamp.org/news/getting-started-with-ner-models-using-huggingface/\n",
    "\n",
    "https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_ner.ipynb#scrollTo=IV72GFgq_ZYb\n",
    "\n",
    "https://github.com/nlpyang/BertSum\n",
    "\n",
    "https://github.com/abhimishra91/transformers-tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import tqdm\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient(os.getenv(\"MONGO_CONNECTION_STRING\"))\n",
    "db = client[\"dragonfly\"]\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from train.dragonfly_dataset.src import DragonflyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "MODEL_NAME = \"distilbert-base-cased\"\n",
    "SEQ_MAX_LENGTH = 512\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:DragonflyDataset:Downloading raw data from Dragonfly collection.\n",
      "10it [00:00, 13.69it/s]\n",
      "INFO:DragonflyDataset:Tokenizing text.\n",
      "INFO:DragonflyDataset:Bulding NER tags.\n",
      "1it [00:00, 1985.00it/s]\n"
     ]
    }
   ],
   "source": [
    "dt = DragonflyDataset(tokenizer, SEQ_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dt.dataset[\"inputs\"][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m tok, tag \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(tokenizer\u001b[39m.\u001b[39mconvert_ids_to_tokens(dt\u001b[39m.\u001b[39mdataset[\u001b[39m\"\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m1\u001b[39m]), dt\u001b[39m.\u001b[39;49mdataset[\u001b[39m\"\u001b[39;49m\u001b[39mtags\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m1\u001b[39;49m]):\n\u001b[1;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(tok,tag)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for tok, tag in zip(tokenizer.convert_ids_to_tokens(dt.dataset[\"inputs\"][\"input_ids\"][1]), dt.dataset[\"tags\"][1]):\n",
    "    print(tok,tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'CRYPTO',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  8667,  1142,   101,  1110, 22620, 16242,   119,   102,     0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Hello',\n",
       " 'this',\n",
       " '[CLS]',\n",
       " 'is',\n",
       " 'spa',\n",
       " '##rta',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer(\n",
    "    [\"Hello this [CLS] is sparta.\"],\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length = 10,\n",
    "    return_offsets_mapping=True\n",
    "    )\n",
    "print(tokens['input_ids'])\n",
    "tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CryptoDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extraction of the latest scraped (reddit) data from the databse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13560it [00:31, 436.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# O, I-CRYPTO, CRYPTO\n",
    "dataset[\"tags\"] = []\n",
    "for token_ids, offsets, matches in tqdm.tqdm(zip(dataset[\"inputs\"][\"input_ids\"],dataset[\"inputs\"][\"offset_mapping\"], dataset[\"ner_matches\"])):\n",
    "    tags = []\n",
    "    for token,offset in zip(tokenizer.convert_ids_to_tokens(token_ids),offsets):\n",
    "        if token in [\"[CLS]\",\"[SEP]\",\"[PAD]\"]:\n",
    "            tags.append(\"O\")\n",
    "            continue\n",
    "        tag = \"O\"\n",
    "        for match in matches:\n",
    "            if tag == \"O\":\n",
    "                # print(token,offset,match[\"span\"][0],offset[0] == match[\"span\"][0],offset[0] > match[\"span\"][0] and offset[1] <= match[\"span\"][1])\n",
    "                if offset[0] == match[\"span\"][0]:\n",
    "                    tag = \"I-CRYPTO\"\n",
    "                elif offset[0] > match[\"span\"][0] and offset[1] <= match[\"span\"][1]:\n",
    "                    tag = \"CRYPTO\"\n",
    "                else:\n",
    "                    tag = \"O\"\n",
    "        tags.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2id = {\n",
    "    \"O\":0,\n",
    "    \"I-CRYPTO\":1,\n",
    "    \"CRYPTO\":2\n",
    "}\n",
    "dataset[\"encoded_tags\"] = [tag2id(x) for x in dataset[\"tags\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'ner_matches', 'inputs', 'tags', 'encoded_tags'])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0],\n",
       "        [  0,   3],\n",
       "        [  4,  11],\n",
       "        [ 12,  15],\n",
       "        [ 16,  20],\n",
       "        [ 21,  23],\n",
       "        [ 24,  28],\n",
       "        [ 29,  32],\n",
       "        [ 33,  35],\n",
       "        [ 36,  38],\n",
       "        [ 39,  42],\n",
       "        [ 43,  47],\n",
       "        [ 48,  52],\n",
       "        [ 53,  56],\n",
       "        [ 57,  65],\n",
       "        [ 66,  68],\n",
       "        [ 69,  71],\n",
       "        [ 71,  75],\n",
       "        [ 76,  79],\n",
       "        [ 79,  80],\n",
       "        [ 80,  81],\n",
       "        [ 82,  86],\n",
       "        [ 87,  89],\n",
       "        [ 89,  91],\n",
       "        [ 92,  94],\n",
       "        [ 96,  98],\n",
       "        [ 98, 102],\n",
       "        [103, 105],\n",
       "        [106, 113],\n",
       "        [113, 114],\n",
       "        [114, 116],\n",
       "        [116, 120],\n",
       "        [120, 121],\n",
       "        [122, 128],\n",
       "        [129, 131],\n",
       "        [132, 133],\n",
       "        [133, 135],\n",
       "        [135, 136],\n",
       "        [  0,   0],\n",
       "        [  0,   0]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offsets[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] O\n",
      "The O\n",
      "biggest O\n",
      "red O\n",
      "flag O\n",
      "as O\n",
      "such O\n",
      "for O\n",
      "me O\n",
      "is O\n",
      "the O\n",
      "fact O\n",
      "that O\n",
      "the O\n",
      "projects O\n",
      "on O\n",
      "Co O\n",
      "##smos O\n",
      "don O\n",
      "’ O\n",
      "t O\n",
      "need O\n",
      "AT I-CRYPTO\n",
      "##OM CRYPTO\n",
      "or O\n",
      "Co O\n",
      "##smos O\n",
      "to O\n",
      "survive O\n",
      "/ O\n",
      "ex O\n",
      "##sist O\n",
      ", O\n",
      "unlike O\n",
      "on O\n",
      "D I-CRYPTO\n",
      "##OT CRYPTO\n",
      ". O\n",
      "[SEP] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n",
      "[PAD] O\n"
     ]
    }
   ],
   "source": [
    "for token, tag in zip(tokenizer.convert_ids_to_tokens(input_ids), tags):\n",
    "    print(token,tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|███████████████████████████| 28.0/28.0 [00:00<00:00, 18.1kB/s]\n",
      "Downloading: 100%|██████████████████████████████| 483/483 [00:00<00:00, 196kB/s]\n",
      "Downloading: 100%|████████████████████████████| 226k/226k [00:00<00:00, 763kB/s]\n",
      "Downloading: 100%|████████████████████████████| 455k/455k [00:00<00:00, 798kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "def tokenize(raw_texts : List[str], tokenizer):\n",
    "    return tokenizer(raw_texts, padding=\"max_length\", truncation=True)\n",
    "tokenized_texts = tokenize([d[\"raw_text\"] for d in dataset],tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(dataset):\n",
    "    ner_tags = []\n",
    "    character_count = 0\n",
    "    token_i = 0\n",
    "    for token in tokenized_texts[i].tokens:\n",
    "        new_tag = 0\n",
    "        if token not in [\"[PAD]\",\"[CLS]\",\"[SEP]\"]:\n",
    "            for tag in data[\"ner_matches\"]:\n",
    "                if token == tag[\"match\"].lower():\n",
    "                    new_tag = 1\n",
    "        ner_tags.append(new_tag)\n",
    "    data[\"ner_tags\"] = ner_tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'raw_text': 'The biggest red flag as such for me is the fact that the projects on Cosmos don’t need ATOM or  Cosmos to survive/exsist, unlike on DOT.',\n",
       " 'ner_tags': [{'symbol': 'DOT', 'match': 'DOT', 'span': [132, 135]},\n",
       "  {'symbol': 'ATOM', 'match': 'ATOM', 'span': [87, 91]}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. BERT pre-processing: tookenization and building labels"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "294a1c64138946dea06e962a84f4621c3846e3b52ebf4546437821a75b530071"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('crypto-ner')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
